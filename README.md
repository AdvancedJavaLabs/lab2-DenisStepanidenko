[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-22041afd0340ce965d47ae6ef1cefeee28c7c493a6346c4f15d667ab976d596c.svg)](https://classroom.github.com/a/QODoQuhO)
# Распределенная обработка текстовых данных с использованием брокера сообщений

## Цель задания:
Реализовать распределённую систему обработки текстовых данных, где секции текста рассылаются на обработку через брокер сообщений (message broker). Несколько воркеров параллельно обрабатывают секции и отправляют результаты на агрегатор. Воркеры выполняют набор задач

## Шаги выполнения:

### Рекомендуемый брокер
RabbitMQ --- прост в развёртывании, понятная модель exchange/queue, есть клиенты для Java и C++. Подходит для предлагаемой задачи.

### Подготовка данных:
Загрузите или создайте набор текстовых данных. Это могут быть, например, книги, статьи или большой корпус текста. Разделите данные на секции для распределения между узлами.

### Разработка приложения:
Общая задача: Необходимо решить следующие задачи для обработки текстовых данных:
* Подсчёт количества слов.
* Поиск N наиболее часто встречающихся слов (top-N).
* Простой анализ тональности — выбрать и реализовать один из подходов:
  * Лексиконный (словарь положительных/отрицательных слов) — прост в реализации. 
  * Наивный байес/предобученная модель.
* Замена всех имён в тексте на заданное подстановку. Для простоты можно:
  * Использовать регулярные выражения (заглавные слова, контекст) или 
  * Подключить лёгкую NER-библиотеку (в Java — OpenNLP или StanfordNLP — опционально).
* Сортировка предложений по длине (в символах) и возврат отсортированного списка.

### Структура системы (компоненты)
1. Producer / Splitter 
   * Читает корпус, разбивает на секции (например, по параграфам, по N предложений или по байтам). 
   * Отправляет задания в очередь/exchange (сообщения с id задания и секцией текста).

2. Worker (несколько экземпляров)
   * Подписывается на очередь задач. 
   * Обрабатывает секцию и отправляет результат в очередь результатов/на агрегатор.

3. Aggregator / Collector
   * Получает частичные результаты от всех воркеров. 
   * Агрегирует: суммирует word counts, объединяет топ-N (merge топов), усредняет/агрегирует тональность, собирает модифицированный текст/заменённые имена (если нужно), объединяет отсортированные предложения (опционально — сохраняет per-section).

4. Result sink / storage 
   * Сохраняет финальные результаты в файл/JSON для отчёта.

### Формат сообщений можете придумать самостоятельно


## Эксперименты и анализ результатов:
Оцените масштабируемость приложения. Используйте различные объемы данных и количество воркеров для определения, насколько эффективно приложение масштабируется.

# Решение

Диаграмма контейнеров (C4 нотация)

<img width="463" height="1369" alt="Диаграмма-контейнеров" src="https://github.com/user-attachments/assets/34a905d8-096f-4981-a897-49cbd2493917" />

# Оценка масштабируемости задачи **"Подсчёт количества слов"**.

<img width="845" height="552" alt="Масштабируемость подсчёта слов" src="https://github.com/user-attachments/assets/d755e4e7-325a-4e5f-9e4f-699f60653d14" />

# Оценка масштабируемости задачи **"Поиск N наиболее часто встречающихся слов (top-N)"**.
Для данного эксперимента возьмём N = 25

<img width="989" height="590" alt="Время выполнения задачи Top-25 слов" src="https://github.com/user-attachments/assets/c6c01752-b345-4cf7-9bbc-1c12d3bde4b4" />

# Оценка масштабируемости задачи **Сортировка предложений по количеству символов**.

<img width="989" height="590" alt="Сортировка предложений по кол-ву символов" src="https://github.com/user-attachments/assets/ffcab6d6-9b16-4ee8-aa17-59abf9dcf9ca" />






